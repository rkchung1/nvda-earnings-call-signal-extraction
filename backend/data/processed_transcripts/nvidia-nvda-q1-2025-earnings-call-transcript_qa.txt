Questions Answers:
Operator We'll pause for just a moment to compile the Q A roster. As a reminder, please limit yourself to one question. Your first question comes from the line of Stacy Rasgon with Bernstein. Please go ahead.
Stacy Rasgon
AllianceBernstein -- Analyst
Hi, guys. Thanks for taking my questions. My first one, I wanted to drill a little bit into the Blackwell comment that it's in full production now. What does that suggest with regard to shipments and delivery timing if that product is -- it doesn't sound like it's sampling anymore.
What does that mean when that's actually in customers' hands if it's in production now?
Jensen Huang
President and Chief Operating Officer
We will be shipping -- well, we've been in production for a little bit of time. But our production shipments will start in Q2 and ramp in Q3, and customers should have data centers stood up in Q4.
Stacy Rasgon
AllianceBernstein -- Analyst
Got it. So, this year, we will see Blackwell revenue, it sounds like.
Jensen Huang
President and Chief Operating Officer
We will see a lot of Blackwell revenue this year.
Operator
Our next question will come from the line of Timothy Arcuri with UBS. Please go ahead.
Tim Arcuri
UBS -- Analyst
Thanks a lot. I wanted to ask, Jensen, about the deployment of Blackwell versus Hopper just given the system's nature and all the demand for GB that you have. How does the deployment of this stuff differ from Hopper? I guess I ask because liquid cooling at scale hasn't been done before, and there's some engineering challenges both at the node level and within the data center. So, do these complexities sort of elongate the transition? And how do you sort of think about how that's all going? Thanks.
Jensen Huang
President and Chief Operating Officer
Yep. Blackwell comes in many configurations. Blackwell is a platform, not a GPU. And the platform includes support for air-cooled, liquid-cooled, x86 and Grace, InfiniBand, now Spectrum-X, and very large NVLink domain that I demonstrated at GTC -- that I showed at GTC.
And so, for some customers, they will ramp into their existing installed base of data centers that are already shipping Hoppers. They will easily transition from H100 to H200 to B100. And so, Blackwell systems have been designed to be backwards compatible, if you will, electrically, mechanically. And of course, the software stack that runs on Hopper will run fantastically on Blackwell.
We also have been priming the pump, if you will, with the entire ecosystem, getting them ready for liquid cooling. We've been talking to the ecosystem about Blackwell for quite some time. And the CSPs, the data centers, the ODMs, the system makers, our supply chain; beyond them, the cooling supply chain base, liquid cooling supply chain base, data center supply chain base, no one is going to be surprised with Blackwell coming and the capabilities that we would like to deliver with Grace Blackwell 200. GB200 is going to be exceptional.
Operator
Our next question will come from the line of Vivek Arya with Bank of America Securities. Please go ahead.
Vivek Arya
Bank of America Merrill Lynch -- Analyst
Thanks for taking my question. Jensen, how are you ensuring that there is enough utilization of your products and that there isn't a pull ahead or a holding behavior because of tight supply, competition, or other factors? Basically, what checks have you built in the system to give us confidence that monetization is keeping pace with your really very strong shipment growth?
Jensen Huang
President and Chief Operating Officer
Well, I guess there's the big picture view that I'll come to, but I'll answer your question directly. The demand for GPUs in all the data centers is incredible. We're racing every single day. And the reason for that is because applications like ChatGPT and GPT-4o, and now, it's going to be multi-modality, Gemini and its ramp and Anthropic, and all of the work that's being done at all the CSPs are consuming every GPU that's out there.
There's also a long line of generative AI start-ups, some 15,000, 20,000 start-ups that are in all different fields, from multimedia to digital characters, of course, all kinds of design tool application, productivity applications, digital biology, the moving of the AV industry to video so that they can train end-to-end models to expand the operating domain of self-driving cars, the list is just quite extraordinary. We're racing actually. Customers are putting a lot of pressure on us to deliver the systems and stand those up as quickly as possible. And of course, I haven't even mentioned all of the sovereign AIs who would like to train all of their regional natural resource of their country, which is their data, to train their regional models.
And there's a lot of pressure to stand those systems up. So, anyhow, the demand, I think, is really, really high and it outstrips our supply. That's the reason why I jumped in to make a few comments. Longer term, we're completely redesigning how computers work.
And this is a platform shift. Of course, it's been compared to other platform shifts in the past. But time will clearly tell that this is much, much more profound than previous platform shifts. And the reason for that is because the computer is no longer an instruction-driven-only computer.
It's an intention-understanding computer. And it understands, of course, the way we interact with it, but it also understands our meaning, what we intend that we asked it to do. And it has the ability to reason, inference iteratively to process a plan, and come back with a solution. And so, every aspect of the computer is changing in such a way that instead of retrieving prerecorded files, it is now generating contextually relevant intelligent answers.
And so, that's going to change computing stacks all over the world. And you saw a build that, in fact, even the PC computing stack is going to get revolutionized. And this is just the beginning of all the things that -- what people see today are the beginning of the things that we're working in our labs and the things that we're doing with all the start-ups and large companies and developers all over the world. It's going to be quite extraordinary.
Operator
Our next question will come from the line of Joe Moore with Morgan Stanley. Please go ahead.
Joe Moore
Morgan Stanley -- Analyst
Great. Thank you. Understanding what you just said about how strong demand is, you have a lot of demand for H200 and for Blackwell products. Do you anticipate any kind of pause with Hopper and H100 as you sort of migrate to those products? Will people wait for those new products, which would be a good product to have? Or do you think there's enough demand for H100 to sustain growth?
Jensen Huang
President and Chief Operating Officer
We see increasing demand of Hopper through this quarter. And we expect demand to outstrip supply for some time as we now transition to H200, as we transition to Blackwell. Everybody is anxious to get their infrastructure online. And the reason for that is because they're saving money and making money, and they would like to do that as soon as possible.
Operator
Our next question will come from the line of Toshiya Hari with Goldman Sachs. Please go ahead.
Toshiya Hari
Goldman Sachs -- Analyst
Hi. Thank you so much for taking the question. Jensen, I wanted to ask about competition. I think many of your cloud customers have announced new or updates to their existing internal programs, right, in parallel to what they're working on with you guys.
To what extent did you consider them as competitors, medium to long term? And in your view, do you think they're limited to addressing mostly internal workloads, or could they be broader in what they address going forward? Thank you.
Jensen Huang
President and Chief Operating Officer
Yeah. We're different in several ways. First, NVIDIA's accelerated computing architecture allows customers to process every aspect of their pipeline from unstructured data processing to prepare it for training, to structured data processing, data frame processing like SQL to prepare for training, to training to inference. And as I was mentioning in my remarks, that inference has really fundamentally changed, it's now generation.
It's not trying to just detect the cat, which was plenty hard in itself, but it has to generate every pixel of a cat. And so, the generation process is a fundamentally different processing architecture. And it's one of the reasons why TensorRT-LLM was so well received. We improved the performance in using the same chips on our architecture by a factor of three.
That kind of tells you something about the richness of our architecture and the richness of our software. So, one, you could use NVIDIA for everything, from computer vision to image processing, to computer graphics, to all modalities of computing. And as the world is now suffering from computing cost and computing energy inflation because general-purpose computing has run its course, accelerated computing is really the sustainable way of going forward. So, accelerated computing is how you're going to save money in computing, is how you're going to save energy in computing.
And so, the versatility of our platform results in the lowest TCO for their data centers. Second, we're in every cloud. And so, for developers that are looking for a platform to develop on, starting with NVIDIA is always a great choice. And we're on-prem.
We're in the cloud. We're in computers of any size and shape. We're practically everywhere. And so, that's the second reason.
The third reason has to do with the fact that we build AI factories. And this is becoming more apparent to people that AI is not a chip problem only. It starts, of course, with very good chips and we build a whole bunch of chips for our AI factories, but it's a systems problem. In fact, even AI is now a systems problem.
It's not just one large language model. It's a complex system of a whole bunch of large language models that are working together. And so, the fact that NVIDIA builds this system causes us to optimize all of our chips to work together as a system, to be able to have software that operates as a system, and to be able to optimize across the system. And just to put it in perspective, in simple numbers, if you had a 5 billion infrastructure and you improved the performance by a factor of two, which we routinely do, when you improve the infrastructure by a factor of two, the value to you is 5 billion.
All the chips in that data center doesn't pay for it. And so, the value of it is really quite extraordinary. And this is the reason why today, performance matters in everything. This is at a time when the highest performance is also the lowest cost because the infrastructure cost of carrying all of these chips cost a lot of money.
And it takes a lot of money to fund the data center, to operate the data center, the people that goes along with it, the power that goes along with it, the real estate that goes along with it, and all of it adds up. And so, the highest performance is also the lowest TCO.
Operator
Our next question will come from the line of Matt Ramsay with TD Cowen. Please go ahead.
Matt Ramsay
TD Cowen -- Analyst
Thank you very much. Good afternoon, everyone. Jensen, I've been in the data center industry my whole career. I've never seen the velocity that you guys are introducing new platforms at the same combination of the performance jumps that you're getting, I mean, 5x in training, some of the stuff you talked about at GTC up to 30x in inference.
And it's an amazing thing to watch but it also creates an interesting juxtaposition where the current generation of product that your customers are spending billions of dollars on is going to be not as competitive with your new stuff very, very much more quickly than the depreciation cycle of that product. So, I'd like you to, if you wouldn't mind, speak a little bit about how you're seeing that situation evolve itself with customers. As you move to Blackwell, they're going to have very large installed bases, obviously software compatible, but large installed bases of product that's not nearly as performant as your new generation stuff. And it'd be interesting to hear what you see happening with customers along that path.
Thank you.
Jensen Huang
President and Chief Operating Officer
Yeah, really appreciate it. Three points that I'd like to make. If you're 5 into the build-out versus if you're 95 into the build-out, you're going to feel very differently. And because you're only 5 into the build-out anyhow, you build as fast as you can.
And when Blackwell comes, it's going to be terrific. And then after Blackwell, as you mentioned, we have other Blackwells coming. And then there's a short -- we're in a one-year rhythm as we've explained to the world. And we want our customers to see our road map for as far as they like, but they're early in their build-out anyways and so they had to just keep on building, OK? And so, there's going to be a whole bunch of chips coming at them, and they just got to keep on building and just, if you will, performance-average your way into it.
So, that's the smart thing to do. They need to make money today. They want to save money today. And time is really, really valuable to them.
Let me give you an example of time being really valuable, why this idea of standing up a data center instantaneously is so valuable, and getting this thing called time-to-train is so valuable. The reason for that is because the next company who reaches the next major plateau gets to announce a groundbreaking AI. And the second one after that gets to announce something that's 0.3 better. And so, the question is, do you want to be repeatedly the company delivering groundbreaking AI or the company delivering 0.3 better? And that's the reason why this race, as in all technology races, the race is so important.
And you're seeing this race across multiple companies because this is so vital to have technology leadership, for companies to trust the leadership that want to build on your platform and know that the platform that they're building on is going to get better and better. And so, leadership matters a great deal. Time-to-train matters a great deal. The difference between time-to-train that is three months earlier just to get it done, in order to get time-to-train on three months' project, getting started three months earlier is everything.
And so, it's the reason why we're standing up Hopper systems like mad right now because the next plateau is just around the corner. And so, that's the second reason. The first comment that you made is really a great comment, which is how is it that we're moving so fast and advancing them quickly, because we have all the stacks here. We literally build the entire data center, and we can monitor everything, measure everything, optimize across everything.
We know where all the bottlenecks are. We're not guessing about it. We're not putting up PowerPoint slides that look good. We're actually -- we also like our PowerPoint slides to look good, but we're delivering systems that perform at scale.
And the reason why we know they perform at scale is because we built it all here. Now, one of the things that we do that's a bit of a miracle is that we build entire AI infrastructure here but then we disaggregate it and integrate it into our customers' data centers however they liked. But we know how it's going to perform, and we know where the bottlenecks are. We know where we need to optimize with them, and we know where we have to help them improve their infrastructure to achieve the most performance.
This deep intimate knowledge at the entire data center scale is fundamentally what sets us apart today. We build every single chip from the ground up. We know exactly how processing is done across the entire system. And so, we understand exactly how it's going to perform and how to get the most out of it with every single generation.
So, I appreciate it. Those are the three points.
Operator
Your next question will come from the line of Mark Lipacis with Evercore ISI. Please go ahead.
Mark Lipacis
Evercore ISI -- Analyst
Hi. Thanks for taking my question. Jensen, in the past, you've made the observation that general-purpose computing ecosystems typically dominated each computing era. And I believe the argument was that they could adapt to different workloads, get higher utilization, drive cost of compute cycle down.
And this is a motivation for why you were driving to a general-purpose GPU CUDA ecosystem for accelerated computing. And if I mischaracterized that observation, please do let me know. So, the question is, given that the workloads that are driving demand for your solutions are being driven by neural network training and inferencing, which on the surface seem like a limited number of workloads, then it might also seem to lend themselves to custom solutions. And so, then the question is, does the general-purpose computing framework become more at risk or is there enough variability or a rapid enough evolution on these workloads that support that historical general-purpose framework? Thank you.
Jensen Huang
President and Chief Operating Officer
Yeah. NVIDIA's accelerated computing is versatile, but I wouldn't call it general purpose. Like, for example, we wouldn't be very good at running the spreadsheet. That was really designed for general-purpose computing.
And so, the control loop of an operating system code probably isn't fantastic for general-purpose computing, not for accelerated computing. And so, I would say that we're versatile, and that's usually the way I describe it. There's a rich domain of applications that we're able to accelerate over the years, but they all have a lot of commonalities, maybe some deep differences, but commonalities. They're all things that I can run in parallel, they're all heavily threaded.
5 of the code represents 99 of the run time, for example. Those are all properties of accelerated computing. The versatility of our platform and the fact that we design entire systems is the reason why over the course of the last 10 years or so, the number of start-ups that you guys have asked me about in these conference calls is fairly large. And every single one of them, because of the brittleness of their architecture, the moment generative AI came along or the moment the fusion models came along, the moment the next models are coming along now, and now all of a sudden, look at this, large language models with memory because the large language model needs to have memory so they can carry on a conversation with you, understand the context.
All of a sudden, the versatility of the Grace memory became super important. And so, each one of these advances in generative AI and the advancement of AI really begs for not having a widget that's designed for one model but to have something that is really good for this entire domain, properties of this entire domain, but obeys the first principles of software: that software is going to continue to evolve, that software is going to keep getting better and bigger. We believe in the scaling of these models. There's a lot of reasons why we're going to scale by easily 1 million times in the coming few years for good reasons, and we're looking forward to it, and we're ready for it.
And so, the versatility of our platform is really quite key. And if you're too brittle and too specific, you might as well just build an FPGA or you build an ASIC or something like that, but that's hardly a computer.
Operator
Our next question will come from the line of Blayne Curtis with Jefferies. Please go ahead.
Blayne Curtis
Jefferies -- Analyst
Thanks for taking my question. I'm actually kind of curious, I mean, being supply constrained, how do you think about -- I mean, you came out with a product for China, H20. I'm assuming there'd be a ton of demand for it, but obviously, you're trying to serve your customers with the other Hopper products. Just kind of curious how you're thinking about that in the second half, if you could elaborate, any impact, what you're thinking for sales as well as gross margin.
Jensen Huang
President and Chief Operating Officer
I didn't hear your questions. Something bleeped out.
Simona Jankowski
Vice President, Investor Relations
H20 and how you're thinking about allocating supply between the different Hopper products.
Jensen Huang
President and Chief Operating Officer
Well, you know, we have customers that we honor and we do our best for every customer. It is the case that our business in China is substantially lower than the levels of the past. And it's a lot more competitive in China now because of the limitations on our technology. And so, those matters are true.
However, we continue to do our best to serve the customers in the markets there, and to the best of our ability, we'll do our best. But I think overall, the comments that we made about demand outstripping supply is for the entire market and particularly so for H200 and Blackwell toward the end of the year.
Operator
Our next question will come from the line of Srini Pajjuri with Raymond James. Please go ahead.
Srini Pajjuri
Raymond James -- Analyst
Thank you. Jensen, actually more of a clarification on what you said. GB200 systems, it looks like there is a significant demand for systems. Historically, I think you've sold a lot of HGX boards and some GPUs and the systems business was relatively small.
So, I'm just curious, why is it that now you are seeing such a strong demand for systems going forward? Is it just the TCO, or is it something else? Or is it just the architecture? Thank you.
Jensen Huang
President and Chief Operating Officer
Yeah. I appreciate that. In fact, the way we sell GB200 is the same. We disaggregate all of the components that make sense, and we integrate it into computer makers.
We have 100 different computer system configurations that are coming this year for Blackwell. And that is off the charts. Hopper, frankly, had only half, but that's at its peak. It started out with way less than that even.
And so, you're going to see liquid-cooled version, air-cooled version, x86 versions, Grace versions, so on and so forth. There's a whole bunch of systems that are being designed. And they're offered from all of our ecosystem of great partners. Nothing has really changed.
Now, of course, the Blackwell platform has expanded our offering tremendously, the integration of CPUs, and the much more compressed density of computing. Liquid cooling is going to save data centers a lot of money in provisioning power and not to mention to be more energy efficient. And so, it's a much better solution. It's more expansive, meaning that we offer a lot more components of a data center.
And everybody wins. The data center gets much higher performance, networking from networking switches, networking -- of course, NICs. We have Ethernet now so that we can bring AI to a large-scale NVIDIA AI to customers who only know how to operate Ethernet because of the ecosystem that they have. And so, Blackwell is much more expansive.
We have a lot more to offer our customers this generation around.
Operator
Our next question will come from the line William Stein with Truist Securities. Please go ahead.
William Stein
Truist Securities -- Analyst
Great. Thanks for taking my question. Jensen, at some point, NVIDIA decided that while there were reasonably good CPUs available for data center operations, your ARM-based Grace CPU provides some real advantage that made that technology worth to bring to customers, perhaps related to cost or power consumption or technical synergies between Grace and Hopper or Grace and Blackwell. Can you address whether there could be a similar dynamic that might emerge on the client side whereby, while there are very good solutions, you've highlighted that Intel and AMD are very good partners and deliver great products in x86, but there might be some, especially in emerging AI workloads, advantage that NVIDIA can deliver that others have more of a challenge?
Jensen Huang
President and Chief Operating Officer
Well, you mentioned some really good reasons. It is true that for many of the applications, our partnership with x86 partners are really terrific and we build excellent systems together. But Grace allows us to do something that isn't possible with the configuration, the system configuration today. The memory system between Grace and Hopper are coherent and connected.
The interconnect between the two chips -- calling it two chips is almost weird because it's like a superchip. The two of them are connected with this interface that's like at terabytes per second. It's off the charts. And the memory that's used by Grace is LPDDR.
It's the first data center-grade low-power memory. And so, we save a lot of power on every single node. And then finally, because of the architecture, because we can create our own architecture with the entire system now, we could create something that has a really large NVLink domain, which is vitally important to the next-generation large language models for inferencing. And so, you saw that GB200 has a 72-node NVLink domain.
That's like 72 Blackwells connected together into one giant GPU. And so, we needed Grace Blackwells to be able to do that. And so, there are architectural reasons, there are software programming reasons and then there are system reasons that are essential for us to build them that way. And so, if we see opportunities like that, we'll explore it.
And today, as you saw at the build yesterday, which I thought was really excellent, Satya announced the next-generation PCs, Copilot PC, which runs fantastically on NVIDIA's RTX GPUs that are shipping in laptops. But it also supports ARM beautifully. And so, it opens up opportunities for system innovation even for PCs.
Operator
Our last question comes from the line of C.J. Muse with Cantor Fitzgerald. Please go ahead.
C.J. Muse
Cantor Fitzgerald -- Analyst
Yeah, good afternoon. Thank you for taking the question. I guess, Jensen, a bit of a longer-term question. I know Blackwell hasn't even launched yet, but obviously, investors are forward-looking.
And amid rising potential competition from GPUs and custom ASICs, how are you thinking about NVIDIA's pace of innovation? And your million-fold scaling over the last decade, truly impressive, CUDA, precision, Grace, Cohere, and connectivity. When you look forward, what frictions need to be solved in the coming decade? And I guess maybe more importantly, what are you willing to share with us today?
Jensen Huang
President and Chief Operating Officer
Well, I can announce that after Blackwell, there's another chip. And we are on a one-year rhythm. And you can also count on us having new networking technology on a very fast rhythm. We're announcing Spectrum-X for Ethernet.
But we're all in on Ethernet, and we have a really exciting road map coming for Ethernet. We have a rich ecosystem of partners. Dell announced that they're taking Spectrum-X to market. We have a rich ecosystem of customers and partners who are going to announce taking our entire AI factory architecture to market.
And so, for companies that want the ultimate performance, we have InfiniBand computing fabric. InfiniBand is a computing fabric, Ethernet to network. And InfiniBand, over the years, started out as a computing fabric, became a better and better network. Ethernet is a network and with Spectrum-X, we're going to make it a much better computing fabric.
And we're committed, fully committed, to all three links, NVLink computing fabric for single computing domain, to InfiniBand computing fabric, to Ethernet networking computing fabric. And so, we're going to take all three of them forward at a very fast clip. And so, you're going to see new switches coming, new NICs coming, new capability, new software stacks that run on all three of them. New CPUs, new GPUs, new networking NICs, new switches, a mound of chips that are coming.
And all of it -- the beautiful thing is all of it runs CUDA. And all of it runs our entire software stack. So, if you invest today on our software stack, without doing anything at all, it's just going to get faster and faster and faster. And if you invest in our architecture today, without doing anything, it will go to more and more clouds and more and more data centers, and everything just runs.
And so, I think the pace of innovation that we're bringing will drive up the capability, on the one hand, and drive down the TCO on the other hand. And so, we should be able to scale out with the NVIDIA architecture for this new era of computing and start this new industrial revolution where we manufacture not just software anymore, but we manufacture artificial intelligence tokens, and we're going to do that at scale. Thank you.
Operator
That will conclude our question-and-answer session and our call for today.